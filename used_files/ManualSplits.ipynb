{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f005847-f095-4ec4-892c-d50332918a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CellMetadataSyn18485175 = pd.read_csv('CellMetadataSyn18485175.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044d91f4-0fb0-411d-b490-364bb98ceeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CellMatrixSyn18485175 = pd.read_parquet('CellMatrixSyn18485175.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119869c1-26d0-4a78-9aae-bd46ab5468d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in training set: [12 45 36 41  4  6  7 17  8 37 31 18  1 42 13 25 40 30 32  3 10  2 26 34\n",
      " 38 35 22 44 28 15 23  5 21 48 16 19 29 24]\n",
      "Samples in test set: [ 9 20 39 43 11 47 33 46 14 27]\n",
      "Number of unique samples in training set: 38\n",
      "Number of unique samples in test set: 10\n",
      "Training set class balance:\n",
      "alzheimers_or_control\n",
      "0    1106\n",
      "1     544\n",
      "Name: count, dtype: int64\n",
      "Testing set class balance:\n",
      "alzheimers_or_control\n",
      "0    154\n",
      "1    116\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 62407 is out of bounds for axis 0 with size 1650",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m best_params_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Create the fold assignments\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m fold_assignments \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_unique_balanced_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Initialize PredefinedSplit with the created folds\u001b[39;00m\n\u001b[1;32m    116\u001b[0m ps \u001b[38;5;241m=\u001b[39m PredefinedSplit(test_fold\u001b[38;5;241m=\u001b[39mfold_assignments)\n",
      "Cell \u001b[0;32mIn[9], line 96\u001b[0m, in \u001b[0;36mcreate_unique_balanced_folds\u001b[0;34m(train_metadata, n_splits)\u001b[0m\n\u001b[1;32m     94\u001b[0m fold_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((fold_controls, fold_cases))\n\u001b[1;32m     95\u001b[0m fold_metadata \u001b[38;5;241m=\u001b[39m train_metadata[train_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(fold_samples)]\n\u001b[0;32m---> 96\u001b[0m \u001b[43mfold_assignments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfold_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Print details about this fold\u001b[39;00m\n\u001b[1;32m     99\u001b[0m num_controls \u001b[38;5;241m=\u001b[39m (fold_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malzheimers_or_control\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 62407 is out of bounds for axis 0 with size 1650"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, recall_score, precision_score, f1_score, matthews_corrcoef, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming CellMetadataSyn18485175 and CellMatrixSyn18485175 are already loaded and preprocessed\n",
    "\n",
    "# Determine Alzheimer's or control status\n",
    "CellMetadataSyn18485175['alzheimers_or_control'] = CellMetadataSyn18485175['age_first_ad_dx'].notnull().astype(int)\n",
    "\n",
    "# Set cell type of interest\n",
    "cell_type = 'Mic'\n",
    "\n",
    "# Filter metadata for the cell type of interest\n",
    "cell_type_metadata = CellMetadataSyn18485175[CellMetadataSyn18485175['broad.cell.type'] == cell_type]\n",
    "\n",
    "# Extract unique sample names and labels\n",
    "unique_samples = cell_type_metadata[['sample', 'alzheimers_or_control']].drop_duplicates()\n",
    "\n",
    "# Use StratifiedShuffleSplit to ensure balanced class distribution\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform the split\n",
    "for train_index, test_index in sss.split(unique_samples['sample'], unique_samples['alzheimers_or_control']):\n",
    "    sample_train, sample_test = unique_samples['sample'].values[train_index], unique_samples['sample'].values[test_index]\n",
    "\n",
    "print(f'Samples in training set: {sample_train}')\n",
    "print(f'Samples in test set: {sample_test}')\n",
    "print(f'Number of unique samples in training set: {len(sample_train)}')\n",
    "print(f'Number of unique samples in test set: {len(sample_test)}')\n",
    "\n",
    "\n",
    "# Filter metadata based on sample numbers -- these are unique identifiers for each individual\n",
    "train_metadata = cell_type_metadata[cell_type_metadata['sample'].isin(sample_train)]\n",
    "test_metadata = cell_type_metadata[cell_type_metadata['sample'].isin(sample_test)]\n",
    "\n",
    "# Extract cell names for training and testing sets\n",
    "train_cell_names = train_metadata['TAG']\n",
    "test_cell_names = test_metadata['TAG']\n",
    "\n",
    "# Extract gene expression data for training and testing sets\n",
    "X_train = CellMatrixSyn18485175[train_cell_names]\n",
    "X_test = CellMatrixSyn18485175[test_cell_names]\n",
    "\n",
    "# Transpose the data to have cells as rows and genes as columns\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "# Extract labels for training and testing sets\n",
    "y_train = train_metadata.set_index('TAG').loc[train_cell_names, 'alzheimers_or_control']\n",
    "y_test = test_metadata.set_index('TAG').loc[test_cell_names, 'alzheimers_or_control']\n",
    "\n",
    "# Convert y_train and y_test to pandas Series for value_counts method\n",
    "y_train_series = pd.Series(y_train)\n",
    "y_test_series = pd.Series(y_test)\n",
    "\n",
    "# Check the class balance\n",
    "train_class_balance = y_train_series.value_counts()\n",
    "test_class_balance = y_test_series.value_counts()\n",
    "\n",
    "print(f'Training set class balance:\\n{train_class_balance}')\n",
    "print(f'Testing set class balance:\\n{test_class_balance}')\n",
    "\n",
    "#coded that was added today\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "def create_unique_balanced_folds(train_metadata, n_splits=3):\n",
    "    controls = train_metadata[train_metadata['alzheimers_or_control'] == 0]['sample'].unique()\n",
    "    cases = train_metadata[train_metadata['alzheimers_or_control'] == 1]['sample'].unique()\n",
    "    \n",
    "    np.random.shuffle(controls)\n",
    "    np.random.shuffle(cases)\n",
    "    \n",
    "    num_controls_per_fold = len(controls) // n_splits\n",
    "    num_cases_per_fold = len(cases) // n_splits\n",
    "    \n",
    "    # Prepare fold assignment array\n",
    "    fold_assignments = np.full(len(train_metadata), -1)  # Initialize with -1 indicating unassigned\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        start_control = i * num_controls_per_fold\n",
    "        end_control = (i + 1) * num_controls_per_fold\n",
    "        start_case = i * num_cases_per_fold\n",
    "        end_case = (i + 1) * num_cases_per_fold\n",
    "        \n",
    "        fold_controls = controls[start_control:end_control]\n",
    "        fold_cases = cases[start_case:end_case]\n",
    "        \n",
    "        fold_samples = np.concatenate((fold_controls, fold_cases))\n",
    "        fold_metadata = train_metadata[train_metadata['sample'].isin(fold_samples)]\n",
    "        fold_assignments[fold_metadata.index] = i\n",
    "        \n",
    "        # Print details about this fold\n",
    "        num_controls = (fold_metadata['alzheimers_or_control'] == 0).sum()\n",
    "        num_cases = (fold_metadata['alzheimers_or_control'] == 1).sum()\n",
    "        print(f\"Fold {i + 1}:\")\n",
    "        print(f\"  Number of controls: {num_controls}\")\n",
    "        print(f\"  Number of cases: {num_cases}\")\n",
    "        print(f\"  Samples in this fold: {fold_samples}\")\n",
    "        print()\n",
    "    \n",
    "    return fold_assignments\n",
    "\n",
    "# Custom cross-validation strategy using the manually created folds\n",
    "cv_results = []\n",
    "best_params_list = []\n",
    "# Create the fold assignments\n",
    "fold_assignments = create_unique_balanced_folds(train_metadata, n_splits=3)\n",
    "\n",
    "# Initialize PredefinedSplit with the created folds\n",
    "ps = PredefinedSplit(test_fold=fold_assignments)\n",
    "\n",
    "# Initialize LightGBM Classifier\n",
    "clf = lgb.LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "#Parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'num_leaves': [20, 30, 40],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'verbosity': [0, 1, 2]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV with PredefinedSplit\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=ps,  # Use PredefinedSplit for cross-validation\n",
    "    scoring='roc_auc',  # Metric to optimize\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from the search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Best score from the search\n",
    "best_score = grid_search.best_score_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Score (ROC AUC): {best_score}')\n",
    "      \n",
    "for i in range(len(folds)):\n",
    "    fold_val_idx = folds[i]\n",
    "    fold_train_idx = np.concatenate([folds[j] for j in range(len(folds)) if j != i])\n",
    "    \n",
    "    X_fold_train = X_train.loc[fold_train_idx]\n",
    "    y_fold_train = y_train.loc[fold_train_idx]\n",
    "    X_fold_val = X_train.loc[fold_val_idx]\n",
    "    y_fold_val = y_train.loc[fold_val_idx]\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        clf, param_distributions=param_dist, n_iter=30, scoring='neg_log_loss', n_jobs=-1, cv=3, random_state=42\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_fold_train, y_fold_train)\n",
    "    best_params = random_search.best_params_\n",
    "    best_params_list.append(best_params)\n",
    "    \n",
    "    clf_best = lgb.LGBMClassifier(**best_params, class_weight='balanced', random_state=42)\n",
    "    clf_best.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    y_fold_prob = clf_best.predict_proba(X_fold_val)[:, 1]\n",
    "    \n",
    "    fold_roc_auc = roc_auc_score(y_fold_val, y_fold_prob)\n",
    "    cv_results.append(fold_roc_auc)\n",
    "\n",
    "mean_cv_roc_auc = np.mean(cv_results)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the final model on the entire training set with the best parameters found during cross-validation\n",
    "clf_final = lgb.LGBMClassifier(**best_params, class_weight='balanced', random_state=42)\n",
    "clf_final.fit(X_train, y_train)\n",
    "y_prob = clf_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "#Youdens stat\n",
    "youden_stat = []\n",
    "\n",
    "#iterates through the different threshold values\n",
    "#for threshold in thresholds:\n",
    "    #y_pred = (y_prob >= threshold).astype(int)\n",
    "    #f1_scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    tpr = recall_score(y_test, y_pred, pos_label=1)\n",
    "    tnr = recall_score(y_test, y_pred, pos_label=0)\n",
    "    youden_stat.append(tpr + tnr - 1)\n",
    "\n",
    "y_pred_optimal = (y_prob >= optimal_threshold).astype(int)\n",
    "optimal_threshold = thresholds[np.argmax(youden_stat)]\n",
    "\n",
    "# Calculate metrics using the optimal threshold\n",
    "test_accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "test_roc_auc = roc_auc_score(y_test, y_prob)\n",
    "test_avg_precision = average_precision_score(y_test, y_prob)\n",
    "test_recall = recall_score(y_test, y_pred_optimal)\n",
    "test_precision = precision_score(y_test, y_pred_optimal)\n",
    "test_f1 = f1_score(y_test, y_pred_optimal)\n",
    "test_mcc = matthews_corrcoef(y_test, y_pred_optimal)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "\n",
    "# Print results\n",
    "print(f'Cell Type: {cell_type}')\n",
    "print(f'Optimal Threshold: {optimal_threshold}')\n",
    "print(f'Test Set Accuracy: {test_accuracy}')\n",
    "print(f'Test Set ROC AUC: {test_roc_auc}')\n",
    "print(f'Test Set Average Precision: {test_avg_precision}')\n",
    "print(f'Test Set Recall: {test_recall}')\n",
    "print(f'Test Set Precision: {test_precision}')\n",
    "print(f'Test Set F1 Score: {test_f1}')\n",
    "print(f'Test Set MCC: {test_mcc}')\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % test_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a6935-07d5-4ee6-b3ce-7912173580f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
