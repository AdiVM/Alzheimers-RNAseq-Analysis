{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bebb0f9-39d3-4cef-bfbf-21068ab91f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/groups/patel/adithya/scenv/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, average_precision_score, recall_score,\n",
    "    precision_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "from flaml import AutoML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4055d7a4-fe24-4606-b5de-540da1550e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_path = \"/n/groups/patel/adithya/Log_Dir_Maximal_test/\"\n",
    "LOG_FILE_PATH = os.path.expanduser(f'{log_dir_path}experiment_log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd35532-09f8-4a87-93c3-df8220451634",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_parquet('/home/adm808/CellMetadataSyn18485175.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0001d50-35fe-466b-b176-d44c18db8846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metadata is loaded\n",
      "Test metadata is loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Train metadata is loaded\")\n",
    "test_metadata = pd.read_csv('/home/adm808/UpdatedCellMetadataSyn16780177.csv', low_memory=False)\n",
    "print(\"Test metadata is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5bfd2-70a4-4249-8768-1a59884bceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases in training: 26148\n",
      "Number of cases in test: 63354\n",
      "Train and test matrices are loaded\n",
      "                    FO538757.2  SAMD11     NOC2L  KLHL17  PLEKHN1\n",
      "AAACGGGAGATCCCGC.1    0.000000     0.0  0.000000     0.0      0.0\n",
      "AAATGCCTCCAATGGT.1    6.919779     0.0  0.000000     0.0      0.0\n",
      "AACCATGTCAGTGCAT.1   11.637401     0.0  0.000000     0.0      0.0\n",
      "AACCATGTCTGTACGA.1    0.000000     0.0  6.974242     0.0      0.0\n",
      "AACCGCGTCCGCATAA.1    7.831354     0.0  0.000000     0.0      0.0\n",
      "                                      MIR1302-2HG  AL627309.1  AL627309.5  \\\n",
      "MFC_B1_01_Cdx1_pAD0_ATCCACCTCTATCGCC          0.0         0.0         0.0   \n",
      "MFC_B1_01_Cdx1_pAD0_CTTACCGCATCACAAC          0.0         0.0         0.0   \n",
      "MFC_B1_01_Cdx1_pAD0_CCTACCAAGATTACCC          0.0         0.0         0.0   \n",
      "MFC_B1_01_Cdx1_pAD0_GGGCACTGTCCGTGAC          0.0         0.0         0.0   \n",
      "MFC_B1_01_Cdx1_pAD0_GTTCATTTCATGGTCA          0.0         0.0         0.0   \n",
      "\n",
      "                                      AP006222.2  AC114498.1  \n",
      "MFC_B1_01_Cdx1_pAD0_ATCCACCTCTATCGCC         0.0         0.0  \n",
      "MFC_B1_01_Cdx1_pAD0_CTTACCGCATCACAAC         0.0         0.0  \n",
      "MFC_B1_01_Cdx1_pAD0_CCTACCAAGATTACCC         0.0         0.0  \n",
      "MFC_B1_01_Cdx1_pAD0_GGGCACTGTCCGTGAC         0.0         0.0  \n",
      "MFC_B1_01_Cdx1_pAD0_GTTCATTTCATGGTCA         0.0         0.0  \n",
      "Printing dimensionality of X_train and X_test initallly\n",
      "(70634, 17926)\n",
      "(172659, 28781)\n",
      "Printing dimensionality of X_train and X_test post filtering and merging\n",
      "(70634, 6581)\n",
      "(172659, 6581)\n",
      "Starting all features classification\n"
     ]
    }
   ],
   "source": [
    "# Process APOE genotype as categorical -- Hot encoding of apoe_genotype\n",
    "combined_metadata = pd.concat([train_metadata, test_metadata], keys=['train', 'test'])\n",
    "combined_metadata = pd.get_dummies(combined_metadata, columns=[\"apoe_genotype\"])\n",
    "apoe_genotype_columns = [col for col in combined_metadata.columns if col.startswith(\"apoe_genotype_\")]\n",
    "\n",
    "# Split back into train and test metadata\n",
    "train_metadata = combined_metadata.xs('train')\n",
    "test_metadata = combined_metadata.xs('test')\n",
    "\n",
    "# Define Alzheimer's or control status\n",
    "train_metadata = train_metadata.copy()\n",
    "test_metadata = test_metadata.copy()\n",
    "train_metadata['alzheimers_or_control'] = train_metadata['age_first_ad_dx'].notnull().astype(int)\n",
    "test_metadata['alzheimers_or_control'] = test_metadata['age_first_ad_dx'].notnull().astype(int)\n",
    "\n",
    "print(f\"Number of cases in training: {sum(train_metadata['alzheimers_or_control'])}\")\n",
    "print(f\"Number of cases in test: {sum(test_metadata['alzheimers_or_control'])}\")\n",
    "\n",
    "# Function to select and drop missing genes\n",
    "def select_missing_genes(filtered_matrix):\n",
    "    mean_threshold = 1\n",
    "    missingness_threshold = 95\n",
    "\n",
    "    mean_gene_expression = filtered_matrix.mean(axis=0)\n",
    "    missingness = (filtered_matrix == 0).sum(axis=0) / filtered_matrix.shape[0] * 100\n",
    "    null_expression = (missingness > missingness_threshold) & (mean_gene_expression < mean_threshold)\n",
    "    genes_to_drop = filtered_matrix.columns[null_expression].tolist()\n",
    "\n",
    "    return genes_to_drop\n",
    "\n",
    "# Transpose and load gene expression matrices\n",
    "# Load and transpose gene expression matrices\n",
    "train_matrix = pd.read_parquet('/home/adm808/NormalizedCellMatrixSyn18485175.parquet').T\n",
    "test_matrix = pd.read_parquet('/home/adm808/NormalizedCellMatrixSyn16780177.parquet').T\n",
    "print(\"Train and test matrices are loaded\")\n",
    "print(train_matrix.iloc[:, :5].head())\n",
    "print(test_matrix.iloc[:, :5].head())\n",
    "\n",
    "print(\"Printing dimensionality of X_train and X_test initallly\")\n",
    "print(train_matrix.shape)\n",
    "print(test_matrix.shape)\n",
    "\n",
    "# Filter missing genes\n",
    "train_matrix_filtered = train_matrix.drop(select_missing_genes(train_matrix), axis=1)\n",
    "test_matrix_filtered = test_matrix.drop(select_missing_genes(test_matrix), axis=1)\n",
    "\n",
    "# Merge the train and test matrices with their respective metadata files\n",
    "\n",
    "train_data = train_matrix_filtered.merge(\n",
    "    train_metadata[['TAG', 'msex', 'broad.cell.type', 'alzheimers_or_control'] + apoe_genotype_columns],\n",
    "    left_index=True,\n",
    "    right_on='TAG',\n",
    "    how='inner'\n",
    ").set_index('TAG')\n",
    "\n",
    "test_data = test_matrix_filtered.merge(\n",
    "    test_metadata[['TAG', 'msex', 'broad.cell.type', 'alzheimers_or_control'] + apoe_genotype_columns],\n",
    "    left_index=True,\n",
    "    right_on='TAG',\n",
    "    how='inner'\n",
    ").set_index('TAG')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean column names for model compatibility\n",
    "train_data.columns = train_data.columns.str.replace(r'[^A-Za-z0-9_]+', '', regex=True)\n",
    "test_data.columns = test_data.columns.str.replace(r'[^A-Za-z0-9_]+', '', regex=True)\n",
    "\n",
    "# Ensure common genes are used between training and testing sets\n",
    "common_genes = train_data.columns.intersection(test_data.columns)\n",
    "X_train = train_data[common_genes]\n",
    "X_test = test_data[common_genes]\n",
    "\n",
    "# Drop the alzheimers or control column from the dataset\n",
    "X_train = X_train.drop(columns=['alzheimers_or_control'])\n",
    "X_test = X_test.drop(columns=['alzheimers_or_control'])\n",
    "\n",
    "# Map original column names to cleaned names for later interpretability\n",
    "original_columns = common_genes  # Use common genes after filtering\n",
    "cleaned_columns = original_columns.str.replace(r'[^A-Za-z0-9_]+', '', regex=True)\n",
    "column_mapping = dict(zip(cleaned_columns, original_columns))\n",
    "\n",
    "# Define the target variable\n",
    "y_train = train_data['alzheimers_or_control']\n",
    "y_test = test_data['alzheimers_or_control']\n",
    "\n",
    "print(\"Printing dimensionality of X_train and X_test post filtering and merging\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# Run maximal classification experiment on all data\n",
    "output_csv = f'{log_dir_path}maximal_output_log.csv'\n",
    "print(\"Starting all features classification\")\n",
    "maximal_classifier = AutoML()\n",
    "maximal_classifier.fit(\n",
    "    X_train, y_train,\n",
    "    task=\"classification\", time_budget=12000, metric='log_loss',\n",
    "    n_jobs=-1, eval_method='cv', n_splits=10, split_type='stratified',\n",
    "    log_training_metric=True, early_stop=True, seed=239875, estimator_list=['lrl1'],\n",
    "    log_file_name=f\"{log_dir_path}/experiment_log.txt\"\n",
    "            #defined the log file for feature retraining\n",
    ")\n",
    "\n",
    " # Save the full model using joblib\n",
    "lasso_model_path = f\"{log_dir_path}lasso_model_all_features.pkl\"\n",
    "joblib.dump(maximal_classifier.model, lasso_model_path)\n",
    "print(f\"Lasso model saved to {lasso_model_path}\")\n",
    "\n",
    "\n",
    "# Predictions and optimal threshold using Youden's J statistic\n",
    "y_prob_train = maximal_classifier.predict_proba(X_train)[:, 1]\n",
    "y_prob_test = maximal_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "youden_stats = [(recall_score(y_train, (y_prob_train >= t).astype(int)) +\n",
    "                 recall_score(y_train, (y_prob_train >= t).astype(int), pos_label=0) - 1)\n",
    "                for t in thresholds]\n",
    "optimal_threshold = thresholds[np.argmax(youden_stats)]\n",
    "\n",
    "y_pred_train_optimal = (y_prob_train >= optimal_threshold).astype(int)\n",
    "y_pred_test_optimal = (y_prob_test >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'train_accuracy': accuracy_score(y_train, y_pred_train_optimal),\n",
    "    'train_roc_auc': roc_auc_score(y_train, y_prob_train),\n",
    "    'train_avg_precision': average_precision_score(y_train, y_prob_train),\n",
    "    'train_recall': recall_score(y_train, y_pred_train_optimal),\n",
    "    'train_precision': precision_score(y_train, y_pred_train_optimal),\n",
    "    'train_f1': f1_score(y_train, y_pred_train_optimal),\n",
    "    'train_mcc': matthews_corrcoef(y_train, y_pred_train_optimal),\n",
    "    'test_accuracy': accuracy_score(y_test, y_pred_test_optimal),\n",
    "    'test_roc_auc': roc_auc_score(y_test, y_prob_test),\n",
    "    'test_avg_precision': average_precision_score(y_test, y_prob_test),\n",
    "    'test_recall': recall_score(y_test, y_pred_test_optimal),\n",
    "    'test_precision': precision_score(y_test, y_pred_test_optimal),\n",
    "    'test_f1': f1_score(y_test, y_pred_test_optimal),\n",
    "    'test_mcc': matthews_corrcoef(y_test, y_pred_test_optimal)\n",
    "}\n",
    "\n",
    "pd.DataFrame([metrics]).to_csv(output_csv, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358274b4-f343-48ad-9f8f-78d08d770566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for top 100 features and avoid mismatch error\n",
    "print(\"Starting iterative feature importances\")\n",
    "\n",
    "if maximal_classifier.feature_importances_ is not None:\n",
    "    # Retrieve the features actually used by the model\n",
    "    used_features = maximal_classifier.feature_names_in_\n",
    "\n",
    "    # Create the feature importance Series with used features only\n",
    "    feature_importance = pd.Series(maximal_classifier.feature_importances_, index=used_features)\n",
    "\n",
    "    # Get the top 100 features and map them back to original names for interpretability\n",
    "    top_features_cleaned = feature_importance.nlargest(100).index\n",
    "    top_features_original = [column_mapping.get(feature, feature) for feature in top_features_cleaned]\n",
    "    \n",
    "    # --- Start Incremental Evaluation of Top Features ---\n",
    "\n",
    "    # Prepare to store incremental results for top features\n",
    "    incremental_results = []\n",
    "\n",
    "    # Loop through 1 to 100 features, adding one feature at a time\n",
    "    for i in range(1, 101):\n",
    "        # Select the top `i` features\n",
    "        current_features = top_features_cleaned[:i]\n",
    "        X_train_top_i = X_train[current_features]\n",
    "        X_test_top_i = X_test[current_features]\n",
    "\n",
    "        # Train the model on the current subset of top features\n",
    "        incremental_classifier = AutoML()\n",
    "        incremental_classifier.retrain_from_log(\n",
    "            X_train_top_i, y_train,\n",
    "            log_file_name=f\"{log_dir_path}/experiment_log.txt\"\n",
    "        )\n",
    "\n",
    "        # Predict probabilities and apply optimal threshold\n",
    "        y_prob_train_i = incremental_classifier.predict_proba(X_train_top_i)[:, 1]\n",
    "        y_prob_test_i = incremental_classifier.predict_proba(X_test_top_i)[:, 1]\n",
    "        \n",
    "        y_pred_train_i = (y_prob_train_i >= optimal_threshold).astype(int)\n",
    "        y_pred_test_i = (y_prob_test_i >= optimal_threshold).astype(int)\n",
    "\n",
    "        # Record metrics for this iteration\n",
    "        result = {\n",
    "            'num_features': i,\n",
    "            'names_of_features': current_features,\n",
    "            'train_accuracy': accuracy_score(y_train, y_pred_train_i),\n",
    "            'train_roc_auc': roc_auc_score(y_train, y_prob_train_i),\n",
    "            'train_avg_precision': average_precision_score(y_train, y_prob_train_i),\n",
    "            'train_recall': recall_score(y_train, y_pred_train_i),\n",
    "            'train_precision': precision_score(y_train, y_pred_train_i),\n",
    "            'train_f1': f1_score(y_train, y_pred_train_i),\n",
    "            'train_mcc': matthews_corrcoef(y_train, y_pred_train_i),\n",
    "            'test_accuracy': accuracy_score(y_test, y_pred_test_i),\n",
    "            'test_roc_auc': roc_auc_score(y_test, y_prob_test_i),\n",
    "            'test_avg_precision': average_precision_score(y_test, y_prob_test_i),\n",
    "            'test_recall': recall_score(y_test, y_pred_test_i),\n",
    "            'test_precision': precision_score(y_test, y_pred_test_i),\n",
    "            'test_f1': f1_score(y_test, y_pred_test_i),\n",
    "            'test_mcc': matthews_corrcoef(y_test, y_pred_test_i)\n",
    "        }\n",
    "\n",
    "        incremental_results.append(result)\n",
    "\n",
    "    # Convert results to DataFrame and save to CSV\n",
    "    incremental_results_df = pd.DataFrame(incremental_results)\n",
    "    incremental_results_df.to_csv(f'{log_dir_path}incremental_top_features_metrics.csv', index=False)\n",
    "\n",
    "    print(\"Incremental top features model training completed\")\n",
    "\n",
    "\n",
    "print(\"Maximal experiment completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
